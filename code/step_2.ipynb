{"cells":[{"cell_type":"markdown","metadata":{},"source":["### IMPORT LIBRARY"]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-21T02:21:54.679888Z","iopub.status.busy":"2024-08-21T02:21:54.679420Z","iopub.status.idle":"2024-08-21T02:21:54.688974Z","shell.execute_reply":"2024-08-21T02:21:54.687936Z","shell.execute_reply.started":"2024-08-21T02:21:54.679847Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import string\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.cluster import KMeans\n","from nltk.tokenize import word_tokenize\n","import time\n","import openpyxl\n","from openpyxl.styles import Alignment, PatternFill\n","from gensim.models import Word2Vec\n","from transformers import BertTokenizer, BertModel\n","import torch\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{},"source":["# TF IDF"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-21T02:21:54.713071Z","iopub.status.busy":"2024-08-21T02:21:54.712345Z","iopub.status.idle":"2024-08-21T02:22:17.094429Z","shell.execute_reply":"2024-08-21T02:22:17.093451Z","shell.execute_reply.started":"2024-08-21T02:21:54.713033Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Recommendations saved to 'recommendations.xlsx'\n"]}],"source":["def preprocess_text_simple(text):\n","    if pd.isna(text):\n","        return \"\"\n","    text = text.lower()\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","    text = re.sub(r'\\*+', '', text)\n","    return text.strip()\n","\n","def remove_asterisks(text):\n","    if pd.isna(text):\n","        return text\n","    return re.sub(r'\\*+', '', text)\n","\n","def load_and_preprocess_job_data(file_path):\n","    df = pd.read_csv(file_path)\n","    df['title'] = df['title'].apply(remove_asterisks)\n","    df['Combined'] = df['title'].fillna('') + ' ' + df['description_x'].fillna('') + ' ' + df['skills_desc'].fillna('')\n","    df['Combined'] = df['Combined'].apply(preprocess_text_simple)\n","    df = df.fillna(\"Unknown\")\n","    return df.reset_index(drop=True)\n","\n","def vectorize_text(df):\n","    vectorizer = TfidfVectorizer(stop_words='english')\n","    tfidf_matrix = vectorizer.fit_transform(df['Combined'])\n","    return vectorizer, tfidf_matrix\n","\n","def recommend_job(user_input, df, vectorizer, tfidf_matrix, experience_levels=None, work_types=None, name=None):\n","    filtered_df = df.copy()\n","    if experience_levels:\n","        filtered_df = filtered_df[filtered_df['formatted_experience_level'].isin(experience_levels)]\n","    if work_types:\n","        filtered_df = filtered_df[filtered_df['formatted_work_type'].isin(work_types)]\n","    if name and name != 'All':\n","        filtered_df = filtered_df[filtered_df['name'] == name]\n","    \n","    if filtered_df.empty:\n","        return None\n","\n","    user_input_processed = preprocess_text_simple(user_input)\n","    user_tfidf = vectorizer.transform([user_input_processed])\n","    \n","    cosine_similarities = cosine_similarity(user_tfidf, tfidf_matrix[filtered_df.index]).flatten()\n","    \n","    above_zero = cosine_similarities > 0\n","    if not any(above_zero):\n","        return None\n","\n","    threshold = np.percentile(cosine_similarities[above_zero], 95)\n","    \n","    above_threshold = cosine_similarities >= threshold\n","    top_job_indices = np.where(above_threshold)[0]\n","    \n","    top_job_indices = top_job_indices[np.argsort(cosine_similarities[top_job_indices])[::-1]]\n","    \n","    top_jobs = filtered_df.iloc[top_job_indices].copy()\n","    top_jobs.reset_index(drop=True, inplace=True)\n","    \n","    top_jobs['cosine_similarity'] = cosine_similarities[top_job_indices]\n","    \n","    return top_jobs\n","\n","def process_test_cases_and_save(test_cases_file, job_data_file, output_file):\n","    test_cases_df = pd.read_csv(test_cases_file)\n","    \n","    df = load_and_preprocess_job_data(job_data_file)\n","    vectorizer, tfidf_matrix = vectorize_text(df)\n","\n","    wb = openpyxl.Workbook()\n","    ws = wb.active\n","    ws.title = \"Recommendations\"\n","\n","    headers = ['No', 'User Preferences', 'Recommendations', 'Scores']\n","    for col, header in enumerate(headers, start=1):\n","        ws.cell(row=1, column=col, value=header)\n","\n","    row_counter = 2\n","    for idx, row in test_cases_df.iterrows():\n","        user_input = row['Case']\n","        \n","        experience_levels = []\n","        work_types = []\n","        name = 'All'\n","        \n","        recommendations = recommend_job(user_input, df, vectorizer, tfidf_matrix, experience_levels, work_types, name)\n","        \n","        if recommendations is not None and not recommendations.empty:\n","            rec_titles = recommendations['title'].tolist()[:10]\n","            scores = recommendations['cosine_similarity'].round(4).tolist()[:10]\n","        else:\n","            rec_titles = [\"No relevant jobs found\"]\n","            scores = [\"N/A\"]\n","\n","        ws.cell(row=row_counter, column=1, value=idx + 1)\n","        ws.cell(row=row_counter, column=2, value=user_input)\n","        \n","        for rec, score in zip(rec_titles, scores):\n","            ws.cell(row=row_counter, column=3, value=rec)\n","            ws.cell(row=row_counter, column=4, value=score)\n","            row_counter += 1\n","\n","        for _ in range(10 - len(rec_titles)):\n","            ws.cell(row=row_counter, column=3, value=\"\")\n","            ws.cell(row=row_counter, column=4, value=\"\")\n","            row_counter += 1\n","\n","    for row in range(2, ws.max_row, 10):\n","        ws.merge_cells(start_row=row, start_column=1, end_row=row+9, end_column=1)\n","        ws.merge_cells(start_row=row, start_column=2, end_row=row+9, end_column=2)\n","\n","    header_fill = PatternFill(start_color=\"CCCCCC\", end_color=\"CCCCCC\", fill_type=\"solid\")\n","    for cell in ws[1]:\n","        cell.fill = header_fill\n","        cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)\n","\n","    for row in ws.iter_rows(min_row=2, max_row=ws.max_row):\n","        for cell in row:\n","            cell.alignment = Alignment(vertical='top', wrap_text=True)\n","\n","    # Adjust column widths\n","    ws.column_dimensions['A'].width = 5\n","    ws.column_dimensions['B'].width = 50\n","    ws.column_dimensions['C'].width = 50\n","    ws.column_dimensions['D'].width = 15\n","\n","    wb.save(output_file)\n","    print(f\"Recommendations saved to '{output_file}'\")\n","\n","if __name__ == \"__main__\":\n","    test_cases_file = '/kaggle/input/cleans/cleaned_test_case_dataset.csv'\n","    job_data_file = '/kaggle/input/linkedin4/Tahap1_LinkedIn4.csv'\n","    output_file = 'recommendations.xlsx'\n","    \n","    process_test_cases_and_save(test_cases_file, job_data_file, output_file)"]},{"cell_type":"markdown","metadata":{},"source":["# Word2Vec"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-21T02:22:17.102634Z","iopub.status.busy":"2024-08-21T02:22:17.102330Z","iopub.status.idle":"2024-08-21T02:27:24.794921Z","shell.execute_reply":"2024-08-21T02:27:24.793640Z","shell.execute_reply.started":"2024-08-21T02:22:17.102603Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Recommendations saved to 'recommendationsword2vec.xlsx'\n"]}],"source":["nltk.download('punkt')\n","\n","def preprocess_text_simple(text):\n","    if pd.isna(text):\n","        return \"\"\n","    text = text.lower()\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","    text = re.sub(r'\\*+', '', text)  \n","    return text.strip()\n","\n","def remove_asterisks(text):\n","    if pd.isna(text):\n","        return text\n","    return re.sub(r'\\*+', '', text)\n","\n","def load_and_preprocess_job_data(file_path):\n","    df = pd.read_csv(file_path)\n","    df = df.drop_duplicates(subset=['company_id', 'title', 'description_x', 'location', 'url'])\n","    df['title'] = df['title'].apply(remove_asterisks)\n","    df['Combined'] = df['title'].fillna('') + ' ' + df['description_x'].fillna('') + ' ' + df['skills_desc'].fillna('')\n","    df['Combined'] = df['Combined'].apply(preprocess_text_simple)\n","    df['Tokenized'] = df['Combined'].apply(word_tokenize)\n","    df = df.fillna(\"Unknown\")\n","    return df.reset_index(drop=True)\n","\n","def train_word2vec(df):\n","    model = Word2Vec(sentences=df['Tokenized'], vector_size=100, window=5, min_count=1, workers=4)\n","    return model\n","\n","def get_document_vector(doc, model):\n","    words = word_tokenize(doc)\n","    word_vectors = [model.wv[word] for word in words if word in model.wv]\n","    if len(word_vectors) == 0:\n","        return np.zeros(model.vector_size)\n","    return np.mean(word_vectors, axis=0)\n","\n","def vectorize_text(df, model):\n","    doc_vectors = np.array([get_document_vector(doc, model) for doc in df['Combined']])\n","    return doc_vectors\n","\n","def recommend_job(user_input, df, model, doc_vectors, experience_levels=None, work_types=None, name=None):\n","    filtered_df = df.copy()\n","    if experience_levels:\n","        filtered_df = filtered_df[filtered_df['formatted_experience_level'].isin(experience_levels)]\n","    if work_types:\n","        filtered_df = filtered_df[filtered_df['formatted_work_type'].isin(work_types)]\n","    if name and name != 'All':\n","        filtered_df = filtered_df[filtered_df['name'] == name]\n","    \n","    if filtered_df.empty:\n","        return None\n","\n","    user_input_processed = preprocess_text_simple(user_input)\n","    user_vector = get_document_vector(user_input_processed, model)\n","    \n","    cosine_similarities = cosine_similarity([user_vector], doc_vectors[filtered_df.index]).flatten()\n","    \n","    above_zero = cosine_similarities > 0\n","    if not any(above_zero):\n","        return None\n","\n","    threshold = np.percentile(cosine_similarities[above_zero], 95)\n","    \n","    above_threshold = cosine_similarities >= threshold\n","    top_job_indices = np.where(above_threshold)[0]\n","    \n","    top_job_indices = top_job_indices[np.argsort(cosine_similarities[top_job_indices])[::-1]]\n","    \n","    top_jobs = filtered_df.iloc[top_job_indices].copy()\n","    top_jobs.reset_index(drop=True, inplace=True)\n","    \n","    top_jobs['cosine_similarity'] = cosine_similarities[top_job_indices]\n","    \n","    return top_jobs\n","\n","def process_test_cases_and_save(test_cases_file, job_data_file, output_file, model, doc_vectors):\n","    test_cases_df = pd.read_csv(test_cases_file)\n","    \n","    df = load_and_preprocess_job_data(job_data_file)\n","\n","    wb = openpyxl.Workbook()\n","    ws = wb.active\n","    ws.title = \"Recommendations\"\n","\n","    headers = ['No', 'User Preferences', 'Recommendations', 'Scores']\n","    for col, header in enumerate(headers, start=1):\n","        ws.cell(row=1, column=col, value=header)\n","\n","    row_counter = 2\n","    for idx, row in test_cases_df.iterrows():\n","        user_input = row['Case']\n","        \n","        experience_levels = []\n","        work_types = []\n","        name = 'All'\n","        \n","        recommendations = recommend_job(user_input, df, model, doc_vectors, experience_levels, work_types, name)\n","        \n","        if recommendations is not None and not recommendations.empty:\n","            rec_titles = recommendations['title'].tolist()[:10]\n","            scores = recommendations['cosine_similarity'].round(4).tolist()[:10]\n","        else:\n","            rec_titles = [\"No relevant jobs found\"]\n","            scores = [\"N/A\"]\n","\n","        ws.cell(row=row_counter, column=1, value=idx + 1)\n","        ws.cell(row=row_counter, column=2, value=user_input)\n","        \n","        for rec, score in zip(rec_titles, scores):\n","            ws.cell(row=row_counter, column=3, value=rec)\n","            ws.cell(row=row_counter, column=4, value=score)\n","            row_counter += 1\n","\n","        for _ in range(10 - len(rec_titles)):\n","            ws.cell(row=row_counter, column=3, value=\"\")\n","            ws.cell(row=row_counter, column=4, value=\"\")\n","            row_counter += 1\n","\n","    for row in range(2, ws.max_row, 10):\n","        ws.merge_cells(start_row=row, start_column=1, end_row=row+9, end_column=1)\n","        ws.merge_cells(start_row=row, start_column=2, end_row=row+9, end_column=2)\n","\n","    header_fill = PatternFill(start_color=\"CCCCCC\", end_color=\"CCCCCC\", fill_type=\"solid\")\n","    for cell in ws[1]:\n","        cell.fill = header_fill\n","        cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)\n","\n","    for row in ws.iter_rows(min_row=2, max_row=ws.max_row):\n","        for cell in row:\n","            cell.alignment = Alignment(vertical='top', wrap_text=True)\n","\n","    ws.column_dimensions['A'].width = 5\n","    ws.column_dimensions['B'].width = 50\n","    ws.column_dimensions['C'].width = 50\n","    ws.column_dimensions['D'].width = 15\n","\n","    wb.save(output_file)\n","    print(f\"Recommendations saved to '{output_file}'\")\n","\n","if __name__ == \"__main__\":\n","    test_cases_file = '/kaggle/input/cleans/cleaned_test_case_dataset.csv'\n","    job_data_file = '/kaggle/input/joblinkedin/linkedin.csv'\n","    output_file = 'recommendationsword2vec.xlsx'\n","    \n","    df = load_and_preprocess_job_data(job_data_file)\n","    word2vec_model = train_word2vec(df)\n","    doc_vectors = vectorize_text(df, word2vec_model)\n","    \n","    process_test_cases_and_save(test_cases_file, job_data_file, output_file, word2vec_model, doc_vectors)"]},{"cell_type":"markdown","metadata":{},"source":["# BERT"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-21T02:27:24.797444Z","iopub.status.busy":"2024-08-21T02:27:24.797143Z","iopub.status.idle":"2024-08-21T04:28:09.085627Z","shell.execute_reply":"2024-08-21T04:28:09.084768Z","shell.execute_reply.started":"2024-08-21T02:27:24.797417Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Vectorizing documents: 100%|██████████| 15309/15309 [2:00:16<00:00,  2.12it/s]  \n","Processing test cases: 100%|██████████| 50/50 [00:13<00:00,  3.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Recommendations saved to 'recommendations_bert.xlsx'\n"]}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","def preprocess_text_simple(text):\n","    if pd.isna(text):\n","        return \"\"\n","    text = text.lower()\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","    text = re.sub(r'\\*+', '', text)  \n","    return text.strip()\n","\n","def remove_asterisks(text):\n","    if pd.isna(text):\n","        return text\n","    return re.sub(r'\\*+', '', text)\n","\n","def load_and_preprocess_job_data(file_path):\n","    df = pd.read_csv(file_path)\n","    df = df.drop_duplicates(subset=['company_id', 'title', 'description_x', 'location', 'url'])\n","    df['title'] = df['title'].apply(remove_asterisks)\n","    df['Combined'] = df['title'].fillna('') + ' ' + df['description_x'].fillna('') + ' ' + df['skills_desc'].fillna('')\n","    df['Combined'] = df['Combined'].apply(preprocess_text_simple)\n","    df = df.fillna(\"Unknown\")\n","    return df.reset_index(drop=True)\n","\n","def get_bert_embedding(text):\n","    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n","\n","def vectorize_text(df):\n","    doc_vectors = []\n","    for doc in tqdm(df['Combined'], desc=\"Vectorizing documents\"):\n","        doc_vectors.append(get_bert_embedding(doc))\n","    return np.array(doc_vectors)\n","\n","def recommend_job(user_input, df, doc_vectors, experience_levels=None, work_types=None, name=None):\n","    filtered_df = df.copy()\n","    if experience_levels:\n","        filtered_df = filtered_df[filtered_df['formatted_experience_level'].isin(experience_levels)]\n","    if work_types:\n","        filtered_df = filtered_df[filtered_df['formatted_work_type'].isin(work_types)]\n","    if name and name != 'All':\n","        filtered_df = filtered_df[filtered_df['name'] == name]\n","    \n","    if filtered_df.empty:\n","        return None\n","\n","    user_input_processed = preprocess_text_simple(user_input)\n","    user_vector = get_bert_embedding(user_input_processed)\n","    \n","    cosine_similarities = cosine_similarity([user_vector], doc_vectors[filtered_df.index]).flatten()\n","    \n","    above_zero = cosine_similarities > 0\n","    if not any(above_zero):\n","        return None\n","\n","    threshold = np.percentile(cosine_similarities[above_zero], 95)\n","    \n","    above_threshold = cosine_similarities >= threshold\n","    top_job_indices = np.where(above_threshold)[0]\n","    \n","    top_job_indices = top_job_indices[np.argsort(cosine_similarities[top_job_indices])[::-1]]\n","    \n","    top_jobs = filtered_df.iloc[top_job_indices].copy()\n","    top_jobs.reset_index(drop=True, inplace=True)\n","    \n","    top_jobs['cosine_similarity'] = cosine_similarities[top_job_indices]\n","    \n","    return top_jobs\n","\n","def process_test_cases_and_save(test_cases_file, job_data_file, output_file, doc_vectors):\n","    test_cases_df = pd.read_csv(test_cases_file)\n","    \n","    df = load_and_preprocess_job_data(job_data_file)\n","\n","    wb = openpyxl.Workbook()\n","    ws = wb.active\n","    ws.title = \"Recommendations\"\n","\n","    headers = ['No', 'User Preferences', 'Recommendations', 'Scores']\n","    for col, header in enumerate(headers, start=1):\n","        ws.cell(row=1, column=col, value=header)\n","\n","    row_counter = 2\n","    for idx, row in tqdm(test_cases_df.iterrows(), total=len(test_cases_df), desc=\"Processing test cases\"):\n","        user_input = row['Case']\n","        \n","        experience_levels = []\n","        work_types = []\n","        name = 'All'\n","        \n","        recommendations = recommend_job(user_input, df, doc_vectors, experience_levels, work_types, name)\n","        \n","        if recommendations is not None and not recommendations.empty:\n","            rec_titles = recommendations['title'].tolist()[:10]\n","            scores = recommendations['cosine_similarity'].round(4).tolist()[:10]\n","        else:\n","            rec_titles = [\"No relevant jobs found\"]\n","            scores = [\"N/A\"]\n","\n","        ws.cell(row=row_counter, column=1, value=idx + 1)\n","        ws.cell(row=row_counter, column=2, value=user_input)\n","        \n","        for rec, score in zip(rec_titles, scores):\n","            ws.cell(row=row_counter, column=3, value=rec)\n","            ws.cell(row=row_counter, column=4, value=score)\n","            row_counter += 1\n","\n","        for _ in range(10 - len(rec_titles)):\n","            ws.cell(row=row_counter, column=3, value=\"\")\n","            ws.cell(row=row_counter, column=4, value=\"\")\n","            row_counter += 1\n","\n","    for row in range(2, ws.max_row, 10):\n","        ws.merge_cells(start_row=row, start_column=1, end_row=row+9, end_column=1)\n","        ws.merge_cells(start_row=row, start_column=2, end_row=row+9, end_column=2)\n","\n","    header_fill = PatternFill(start_color=\"CCCCCC\", end_color=\"CCCCCC\", fill_type=\"solid\")\n","    for cell in ws[1]:\n","        cell.fill = header_fill\n","        cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)\n","\n","    for row in ws.iter_rows(min_row=2, max_row=ws.max_row):\n","        for cell in row:\n","            cell.alignment = Alignment(vertical='top', wrap_text=True)\n","\n","    ws.column_dimensions['A'].width = 5\n","    ws.column_dimensions['B'].width = 50\n","    ws.column_dimensions['C'].width = 50\n","    ws.column_dimensions['D'].width = 15\n","\n","    wb.save(output_file)\n","    print(f\"Recommendations saved to '{output_file}'\")\n","\n","if __name__ == \"__main__\":\n","    test_cases_file = '/kaggle/input/cleans/cleaned_test_case_dataset.csv'\n","    job_data_file = '/kaggle/input/joblinkedin/linkedin.csv'\n","    output_file = 'recommendations_bert.xlsx'\n","    \n","    df = load_and_preprocess_job_data(job_data_file)\n","    doc_vectors = vectorize_text(df)\n","    \n","    process_test_cases_and_save(test_cases_file, job_data_file, output_file, doc_vectors)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5443200,"sourceId":9030921,"sourceType":"datasetVersion"},{"datasetId":5462709,"sourceId":9058886,"sourceType":"datasetVersion"},{"datasetId":5468166,"sourceId":9066492,"sourceType":"datasetVersion"},{"datasetId":5523284,"sourceId":9144621,"sourceType":"datasetVersion"},{"datasetId":5528272,"sourceId":9151763,"sourceType":"datasetVersion"},{"datasetId":5528303,"sourceId":9151811,"sourceType":"datasetVersion"},{"datasetId":3680745,"sourceId":9200871,"sourceType":"datasetVersion"},{"datasetId":5570405,"sourceId":9212268,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
